{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uXu6D-nv_Z3N",
        "outputId": "ff74bd7c-ae65-4c1e-9dba-c697e1d39e8b",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1696713745514,
          "user_tz": -330,
          "elapsed": 19179,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-chess in /usr/local/lib/python3.10/dist-packages (1.999)\n",
            "Requirement already satisfied: chess<2,>=1 in /usr/local/lib/python3.10/dist-packages (from python-chess) (1.10.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (5.9.5)\n",
            "Requirement already satisfied: google-cloud-storage in /usr/local/lib/python3.10/dist-packages (2.8.0)\n",
            "Requirement already satisfied: google-auth<3.0dev,>=1.25.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage) (2.17.3)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage) (2.11.1)\n",
            "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage) (2.3.3)\n",
            "Requirement already satisfied: google-resumable-media>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage) (2.6.0)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage) (2.31.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage) (1.60.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage) (3.20.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage) (0.3.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage) (1.16.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage) (4.9)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-resumable-media>=2.3.2->google-cloud-storage) (1.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (2023.7.22)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-cloud-storage) (0.5.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install python-chess\n",
        "!pip install psutil\n",
        "!pip install google-cloud-storage\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "import chess\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/content/gleaming-modem-400906-3780658b0f47.json\"\n",
        "from google.cloud import storage\n",
        "\n",
        "def download_from_bucket(blob_name, path_to_save_to, bucket_name):\n",
        "    client = storage.Client()\n",
        "\n",
        "    bucket = client.get_bucket(bucket_name)\n",
        "    blob = bucket.blob(blob_name)\n",
        "    blob.download_to_filename(path_to_save_to)\n",
        "    print(f\"File {blob_name} downloaded to {path_to_save_to}.\")\n",
        "\n",
        "# Use the function to download your file:\n",
        "download_from_bucket(\"final_4M_bits.npy\", \"/content/final_4M_bits.npy\", \"chess_data_all\")\n",
        "download_from_bucket(\"final_4M_labels.npy\", \"/content/final_4M_labels.npy\", \"chess_data_all\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_SV1_hnRi4En",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1696706269080,
          "user_tz": -330,
          "elapsed": 1153272,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "387f7a9d-b4ba-4c8b-f4a2-beba482ee6da"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File final_4M_bits.npy downloaded to /content/final_4M_bits.npy.\n",
            "File final_4M_labels.npy downloaded to /content/final_4M_labels.npy.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "bnfn9WPEni1p",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1696713755896,
          "user_tz": -330,
          "elapsed": 10387,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "outputs": [],
      "source": [
        "bitboard_dataset=np.load(\"/content/final_4M_bits.npy\")\n",
        "labels=np.load(\"/content/final_4M_labels.npy\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bitboard_dataset2=bitboard_dataset[:2000000]\n",
        "labels2=labels[:2000000]\n"
      ],
      "metadata": {
        "id": "wwHRx5f__Krh",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1696713755898,
          "user_tz": -330,
          "elapsed": 14,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(bitboard_dataset2.shape ,bitboard_dataset2[0])\n",
        "# print(labels2.shape ,labels2[0])"
      ],
      "metadata": {
        "id": "CkrhPafx9Sxh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Model\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Input, Conv2D, Conv3D, Flatten, Dense, Concatenate\n",
        "from tensorflow.keras.activations import relu\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import DepthwiseConv2D\n",
        "\n",
        "\n",
        "# ... Assuming bitboard_dataset and labels are defined ...\n",
        "\n",
        "bitboard_dataset, labels = shuffle(bitboard_dataset2, labels2)\n",
        "X_train, X_test, y_train, y_test = train_test_split(bitboard_dataset2, labels2, test_size=0.1)\n",
        "\n",
        "\n",
        "# Input layer\n",
        "input_layer_2d = layers.Input(shape=(16,8, 8))  # for 2D convolutions\n",
        "input_layer_3d = layers.Reshape((16, 8, 8, 1))(input_layer_2d)  # for 3D convolutions\n"
      ],
      "metadata": {
        "id": "cjyNKNin14dd",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1696713769371,
          "user_tz": -330,
          "elapsed": 13484,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import BatchNormalization, Dropout, Activation\n",
        "\n",
        "# Branch 1: Depthwise Convolution\n",
        "branch1 = DepthwiseConv2D(kernel_size=(1, 1), depth_multiplier=1, activation=relu)(input_layer_2d)\n",
        "branch1 = BatchNormalization()(branch1)\n",
        "branch1 = Dropout(0.1)(branch1)\n",
        "\n",
        "\n",
        "branch4 = Conv2D(filters=3, kernel_size=(8, 8), activation=relu)(branch1)\n",
        "branch4 = BatchNormalization()(branch4)\n",
        "branch4 = Dropout(0.1)(branch4)\n",
        "# branch4 = Conv2D(filters=5, kernel_size=(2, 2), activation=relu)(branch4)\n",
        "\n",
        "# Branch 2: Conv3D\n",
        "branch2 = Conv3D(filters=16, kernel_size=(5, 5, 5), padding='valid', activation=relu)(input_layer_3d)\n",
        "branch2 = BatchNormalization()(branch2)\n",
        "branch2 = Dropout(0.1)(branch2)\n",
        "branch2 = Conv3D(filters=8, kernel_size=(3, 3, 3), padding='valid', activation=relu)(branch2)\n",
        "branch2 = BatchNormalization()(branch2)\n",
        "\n",
        "\n",
        "\n",
        "# Branch 3: Conv3 (Only to the 15th channel)\n",
        "branch3_input = tf.gather(input_layer_2d, [13,14], axis=3)  # Extract the 15th channel\n",
        "branch3 = DepthwiseConv2D(kernel_size=(1, 1), depth_multiplier=1, activation=relu)(branch3_input)\n",
        "branch3 = Conv2D(filters=10, kernel_size=(8,8), activation=relu)(branch3)\n",
        "branch3 = Conv2D(filters=2, kernel_size=(1, 1), activation=relu)(branch3)\n",
        "branch3 = BatchNormalization()(branch3)\n",
        "branch3 = Dropout(0.1)(branch3)\n",
        "\n",
        "\n",
        "branch1 = Flatten()(branch1)\n",
        "branch2 = Flatten()(branch2)  # Flatten branch2\n",
        "branch4 = Flatten()(branch4)\n",
        "branch3 = Flatten()(branch3)  # Flatten branch3\n",
        "\n",
        "concat = Concatenate()([branch1,branch4, branch2, branch3])\n",
        "\n",
        "# Dense layers\n",
        "dense1 = Dense(2000)(concat)\n",
        "dense1 = BatchNormalization()(dense1)\n",
        "dense1 = Activation('relu')(dense1)\n",
        "dense1 = Dropout(0.25)(dense1)\n",
        "\n",
        "dense2 = Dense(1000)(dense1)\n",
        "dense2 = BatchNormalization()(dense2)\n",
        "dense2 = Activation('relu')(dense2)\n",
        "dense2 = Dropout(0.25)(dense2)\n",
        "\n",
        "dense3 = Dense(500)(dense2)\n",
        "dense3 = BatchNormalization()(dense3)\n",
        "dense3 = Activation('relu')(dense3)\n",
        "dense3 = Dropout(0.25)(dense3)\n",
        "\n",
        "dense4 = Dense(100)(dense3)\n",
        "dense4 = BatchNormalization()(dense4)\n",
        "dense4 = Activation('relu')(dense4)\n",
        "dense4 = Dropout(0.25)(dense4)\n",
        "\n",
        "output_layer = Dense(1)(dense4)\n",
        "\n",
        "# Create the model\n",
        "model = Model(inputs=input_layer_2d, outputs=output_layer)\n",
        "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=200, batch_size=500, validation_split=0.1)\n",
        "\n",
        "# Evaluate the model\n",
        "loss, mae= model.evaluate(X_test, y_test)\n",
        "print(\"Test Loss:\", loss)\n",
        "print(\"Mean Absolute Error:\", mae)"
      ],
      "metadata": {
        "id": "tQeo2FuNIIkp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1696722635894,
          "user_tz": -330,
          "elapsed": 8823998,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "f10d37cd-51af-4133-cbc4-b436f6a173b7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "3240/3240 [==============================] - 55s 14ms/step - loss: 7172.2217 - mae: 33.4682 - val_loss: 6394.1147 - val_mae: 31.1108\n",
            "Epoch 2/200\n",
            "3240/3240 [==============================] - 44s 13ms/step - loss: 6338.1045 - mae: 32.1346 - val_loss: 5914.1353 - val_mae: 30.4273\n",
            "Epoch 3/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 5778.7036 - mae: 31.2157 - val_loss: 5433.6616 - val_mae: 29.0331\n",
            "Epoch 4/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 5224.9761 - mae: 30.2828 - val_loss: 5115.8774 - val_mae: 28.7749\n",
            "Epoch 5/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 4709.9570 - mae: 29.3487 - val_loss: 4802.2246 - val_mae: 27.6896\n",
            "Epoch 6/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 4343.1631 - mae: 28.5800 - val_loss: 4978.7168 - val_mae: 28.3240\n",
            "Epoch 7/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 4004.6963 - mae: 27.8697 - val_loss: 4648.5146 - val_mae: 27.3369\n",
            "Epoch 8/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 3732.2554 - mae: 27.2745 - val_loss: 4393.4253 - val_mae: 26.7832\n",
            "Epoch 9/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 3483.4802 - mae: 26.7068 - val_loss: 4151.2832 - val_mae: 25.7670\n",
            "Epoch 10/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 3305.5222 - mae: 26.2908 - val_loss: 4009.3037 - val_mae: 25.4476\n",
            "Epoch 11/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 3115.8538 - mae: 25.8299 - val_loss: 4201.3193 - val_mae: 25.5740\n",
            "Epoch 12/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 2966.6409 - mae: 25.4292 - val_loss: 4284.0806 - val_mae: 26.4661\n",
            "Epoch 13/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 2843.1799 - mae: 25.0946 - val_loss: 3907.4949 - val_mae: 25.6643\n",
            "Epoch 14/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 2716.0161 - mae: 24.7770 - val_loss: 3875.7700 - val_mae: 24.6127\n",
            "Epoch 15/200\n",
            "3240/3240 [==============================] - 44s 13ms/step - loss: 2631.5129 - mae: 24.5482 - val_loss: 3835.4646 - val_mae: 24.4075\n",
            "Epoch 16/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 2519.6038 - mae: 24.2139 - val_loss: 4413.2080 - val_mae: 29.1598\n",
            "Epoch 17/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 2417.1909 - mae: 23.9473 - val_loss: 3777.4265 - val_mae: 24.9262\n",
            "Epoch 18/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 2367.6865 - mae: 23.7629 - val_loss: 3799.0068 - val_mae: 24.0738\n",
            "Epoch 19/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 2304.6355 - mae: 23.5456 - val_loss: 3649.8809 - val_mae: 23.4194\n",
            "Epoch 20/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 2218.5659 - mae: 23.3360 - val_loss: 3689.9468 - val_mae: 24.2476\n",
            "Epoch 21/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 2171.9497 - mae: 23.1506 - val_loss: 3661.5005 - val_mae: 23.5104\n",
            "Epoch 22/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 2093.5933 - mae: 22.9229 - val_loss: 3619.7053 - val_mae: 23.3424\n",
            "Epoch 23/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 2076.7749 - mae: 22.8359 - val_loss: 3618.9607 - val_mae: 23.3213\n",
            "Epoch 24/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 1997.2771 - mae: 22.5661 - val_loss: 3905.1438 - val_mae: 24.8361\n",
            "Epoch 25/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 1958.0896 - mae: 22.4137 - val_loss: 3544.4473 - val_mae: 22.9054\n",
            "Epoch 26/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 1927.8671 - mae: 22.2948 - val_loss: 3525.3704 - val_mae: 22.9195\n",
            "Epoch 27/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 1867.6891 - mae: 22.0859 - val_loss: 3486.2449 - val_mae: 22.6305\n",
            "Epoch 28/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 1843.4279 - mae: 21.9918 - val_loss: 3507.5667 - val_mae: 22.6003\n",
            "Epoch 29/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 1816.2078 - mae: 21.8498 - val_loss: 3476.6851 - val_mae: 22.4174\n",
            "Epoch 30/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 1796.0087 - mae: 21.7533 - val_loss: 3636.5007 - val_mae: 23.3378\n",
            "Epoch 31/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 1754.0286 - mae: 21.6151 - val_loss: 3455.5051 - val_mae: 22.2990\n",
            "Epoch 32/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 1721.7599 - mae: 21.4963 - val_loss: 3492.2773 - val_mae: 22.5199\n",
            "Epoch 33/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 1690.0699 - mae: 21.3773 - val_loss: 3489.4250 - val_mae: 22.4473\n",
            "Epoch 34/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 1669.1161 - mae: 21.2829 - val_loss: 3541.8535 - val_mae: 22.8427\n",
            "Epoch 35/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 1629.4288 - mae: 21.1278 - val_loss: 3448.1401 - val_mae: 22.1867\n",
            "Epoch 36/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 1605.9496 - mae: 21.0229 - val_loss: 3494.6550 - val_mae: 22.1902\n",
            "Epoch 37/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 1575.3635 - mae: 20.9245 - val_loss: 3423.5032 - val_mae: 21.8951\n",
            "Epoch 38/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 1575.7949 - mae: 20.8538 - val_loss: 3422.5352 - val_mae: 21.7253\n",
            "Epoch 39/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 1541.1416 - mae: 20.7171 - val_loss: 3467.7429 - val_mae: 21.8352\n",
            "Epoch 40/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 1529.4250 - mae: 20.6644 - val_loss: 3509.6565 - val_mae: 21.8560\n",
            "Epoch 41/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 1493.0198 - mae: 20.5445 - val_loss: 3391.6987 - val_mae: 21.8115\n",
            "Epoch 42/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 1474.9225 - mae: 20.4297 - val_loss: 3393.7983 - val_mae: 21.4337\n",
            "Epoch 43/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 1467.8536 - mae: 20.3793 - val_loss: 3431.4312 - val_mae: 21.4964\n",
            "Epoch 44/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 1451.0140 - mae: 20.2789 - val_loss: 3409.3220 - val_mae: 21.8619\n",
            "Epoch 45/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 1439.0396 - mae: 20.2256 - val_loss: 3325.4907 - val_mae: 21.2078\n",
            "Epoch 46/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 1406.9451 - mae: 20.1088 - val_loss: 3378.9841 - val_mae: 21.3126\n",
            "Epoch 47/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 1390.3665 - mae: 20.0199 - val_loss: 3405.7966 - val_mae: 21.3410\n",
            "Epoch 48/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 1397.8459 - mae: 20.0303 - val_loss: 3364.8376 - val_mae: 21.3801\n",
            "Epoch 49/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 1351.4043 - mae: 19.8543 - val_loss: 3345.7642 - val_mae: 21.1817\n",
            "Epoch 50/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 1358.7616 - mae: 19.8187 - val_loss: 3341.7715 - val_mae: 21.0388\n",
            "Epoch 51/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 1330.3879 - mae: 19.7458 - val_loss: 3373.7991 - val_mae: 21.3062\n",
            "Epoch 52/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 1323.0780 - mae: 19.6577 - val_loss: 3357.9243 - val_mae: 20.9023\n",
            "Epoch 53/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 1315.4717 - mae: 19.6026 - val_loss: 3383.8845 - val_mae: 20.9731\n",
            "Epoch 54/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 1308.6611 - mae: 19.5803 - val_loss: 3347.3821 - val_mae: 20.8616\n",
            "Epoch 55/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 1282.4353 - mae: 19.4437 - val_loss: 3287.2878 - val_mae: 20.6512\n",
            "Epoch 56/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 1274.8373 - mae: 19.4185 - val_loss: 3315.6377 - val_mae: 21.0600\n",
            "Epoch 57/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 1273.7614 - mae: 19.3560 - val_loss: 3380.9866 - val_mae: 21.2310\n",
            "Epoch 58/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 1252.0707 - mae: 19.2885 - val_loss: 3265.4817 - val_mae: 20.7018\n",
            "Epoch 59/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 1221.5958 - mae: 19.1797 - val_loss: 3316.1130 - val_mae: 20.6756\n",
            "Epoch 60/200\n",
            "3240/3240 [==============================] - 44s 13ms/step - loss: 1225.2369 - mae: 19.1490 - val_loss: 3301.0767 - val_mae: 20.4469\n",
            "Epoch 61/200\n",
            "3240/3240 [==============================] - 44s 13ms/step - loss: 1212.2817 - mae: 19.1010 - val_loss: 3300.8655 - val_mae: 20.3755\n",
            "Epoch 62/200\n",
            "3240/3240 [==============================] - 44s 13ms/step - loss: 1210.8054 - mae: 19.0520 - val_loss: 3322.4814 - val_mae: 20.8660\n",
            "Epoch 63/200\n",
            "3240/3240 [==============================] - 44s 13ms/step - loss: 1205.9016 - mae: 19.0180 - val_loss: 3402.7007 - val_mae: 21.4862\n",
            "Epoch 64/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 1196.3337 - mae: 18.9455 - val_loss: 3295.8198 - val_mae: 20.4546\n",
            "Epoch 65/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 1177.0164 - mae: 18.8859 - val_loss: 3227.0276 - val_mae: 20.1461\n",
            "Epoch 66/200\n",
            "3240/3240 [==============================] - 44s 13ms/step - loss: 1186.5669 - mae: 18.8679 - val_loss: 3327.0603 - val_mae: 20.2400\n",
            "Epoch 67/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 1172.0726 - mae: 18.8145 - val_loss: 3260.1101 - val_mae: 20.1357\n",
            "Epoch 68/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 1165.0677 - mae: 18.7640 - val_loss: 3234.4194 - val_mae: 20.1505\n",
            "Epoch 69/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 1148.2126 - mae: 18.7037 - val_loss: 3227.1755 - val_mae: 20.2219\n",
            "Epoch 70/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 1130.9420 - mae: 18.6220 - val_loss: 3285.5364 - val_mae: 19.9925\n",
            "Epoch 71/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 1135.5223 - mae: 18.5922 - val_loss: 3268.1880 - val_mae: 20.5075\n",
            "Epoch 72/200\n",
            "3240/3240 [==============================] - 44s 13ms/step - loss: 1117.3184 - mae: 18.5233 - val_loss: 3151.3464 - val_mae: 19.7723\n",
            "Epoch 73/200\n",
            "3240/3240 [==============================] - 44s 13ms/step - loss: 1113.2875 - mae: 18.4855 - val_loss: 3243.1670 - val_mae: 19.9546\n",
            "Epoch 74/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 1114.9905 - mae: 18.5014 - val_loss: 3236.4800 - val_mae: 19.8181\n",
            "Epoch 75/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 1110.0081 - mae: 18.4450 - val_loss: 3251.9321 - val_mae: 19.8962\n",
            "Epoch 76/200\n",
            "3240/3240 [==============================] - 44s 13ms/step - loss: 1095.7126 - mae: 18.3703 - val_loss: 3232.8918 - val_mae: 19.8559\n",
            "Epoch 77/200\n",
            "3240/3240 [==============================] - 44s 13ms/step - loss: 1088.1157 - mae: 18.3396 - val_loss: 3242.8101 - val_mae: 19.7823\n",
            "Epoch 78/200\n",
            "3240/3240 [==============================] - 44s 13ms/step - loss: 1069.0110 - mae: 18.2551 - val_loss: 3251.0710 - val_mae: 19.8811\n",
            "Epoch 79/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 1067.1831 - mae: 18.2348 - val_loss: 3237.5996 - val_mae: 20.0058\n",
            "Epoch 80/200\n",
            "3240/3240 [==============================] - 44s 13ms/step - loss: 1068.7103 - mae: 18.2082 - val_loss: 3227.4077 - val_mae: 19.6213\n",
            "Epoch 81/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 1045.9778 - mae: 18.1219 - val_loss: 3220.4177 - val_mae: 20.4359\n",
            "Epoch 82/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 1055.3306 - mae: 18.1290 - val_loss: 3218.6147 - val_mae: 19.6483\n",
            "Epoch 83/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 1045.8687 - mae: 18.0618 - val_loss: 3206.5852 - val_mae: 19.5884\n",
            "Epoch 84/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 1038.6658 - mae: 18.0459 - val_loss: 3174.4932 - val_mae: 19.6298\n",
            "Epoch 85/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 1028.9037 - mae: 17.9533 - val_loss: 3247.4705 - val_mae: 19.6762\n",
            "Epoch 86/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 1027.8496 - mae: 17.9489 - val_loss: 3170.9553 - val_mae: 19.4797\n",
            "Epoch 87/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 1030.6766 - mae: 17.9631 - val_loss: 3151.7188 - val_mae: 19.4706\n",
            "Epoch 88/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 1013.1329 - mae: 17.8743 - val_loss: 3167.9883 - val_mae: 19.4943\n",
            "Epoch 89/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 1013.2249 - mae: 17.8563 - val_loss: 3226.3835 - val_mae: 19.5666\n",
            "Epoch 90/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 1011.8108 - mae: 17.8206 - val_loss: 3197.0386 - val_mae: 19.7598\n",
            "Epoch 91/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 1002.2721 - mae: 17.7767 - val_loss: 3161.5090 - val_mae: 19.3595\n",
            "Epoch 92/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 1003.8187 - mae: 17.7685 - val_loss: 3231.6812 - val_mae: 19.7088\n",
            "Epoch 93/200\n",
            "3240/3240 [==============================] - 44s 13ms/step - loss: 1002.2162 - mae: 17.7613 - val_loss: 3166.2947 - val_mae: 19.3548\n",
            "Epoch 94/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 995.1882 - mae: 17.7108 - val_loss: 3180.0645 - val_mae: 19.3233\n",
            "Epoch 95/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 968.9804 - mae: 17.6132 - val_loss: 3192.0452 - val_mae: 19.3110\n",
            "Epoch 96/200\n",
            "3240/3240 [==============================] - 44s 13ms/step - loss: 980.5181 - mae: 17.6319 - val_loss: 3206.4077 - val_mae: 19.2551\n",
            "Epoch 97/200\n",
            "3240/3240 [==============================] - 44s 13ms/step - loss: 970.7797 - mae: 17.5772 - val_loss: 3163.6687 - val_mae: 19.2232\n",
            "Epoch 98/200\n",
            "3240/3240 [==============================] - 44s 13ms/step - loss: 967.9004 - mae: 17.5558 - val_loss: 3223.6765 - val_mae: 19.4074\n",
            "Epoch 99/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 962.0140 - mae: 17.5344 - val_loss: 3148.4753 - val_mae: 19.4593\n",
            "Epoch 100/200\n",
            "3240/3240 [==============================] - 44s 13ms/step - loss: 958.4709 - mae: 17.4974 - val_loss: 3151.4111 - val_mae: 19.1174\n",
            "Epoch 101/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 948.3775 - mae: 17.4479 - val_loss: 3195.7126 - val_mae: 19.1897\n",
            "Epoch 102/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 944.2073 - mae: 17.4319 - val_loss: 3156.5703 - val_mae: 19.1040\n",
            "Epoch 103/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 955.1716 - mae: 17.4326 - val_loss: 3153.0310 - val_mae: 19.0658\n",
            "Epoch 104/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 941.4693 - mae: 17.3764 - val_loss: 3140.5071 - val_mae: 19.2125\n",
            "Epoch 105/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 936.2336 - mae: 17.3589 - val_loss: 3154.7053 - val_mae: 18.9950\n",
            "Epoch 106/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 932.8794 - mae: 17.3174 - val_loss: 3120.0874 - val_mae: 19.0420\n",
            "Epoch 107/200\n",
            "3240/3240 [==============================] - 44s 13ms/step - loss: 930.1914 - mae: 17.2922 - val_loss: 3162.0808 - val_mae: 18.9197\n",
            "Epoch 108/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 916.9389 - mae: 17.2561 - val_loss: 3121.7607 - val_mae: 19.0594\n",
            "Epoch 109/200\n",
            "3240/3240 [==============================] - 44s 13ms/step - loss: 922.6031 - mae: 17.2407 - val_loss: 3124.2095 - val_mae: 18.9785\n",
            "Epoch 110/200\n",
            "3240/3240 [==============================] - 44s 13ms/step - loss: 917.8263 - mae: 17.2019 - val_loss: 3154.5369 - val_mae: 19.0048\n",
            "Epoch 111/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 915.8316 - mae: 17.1861 - val_loss: 3160.6379 - val_mae: 18.9336\n",
            "Epoch 112/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 907.3728 - mae: 17.1508 - val_loss: 3133.9468 - val_mae: 18.8906\n",
            "Epoch 113/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 907.1937 - mae: 17.1286 - val_loss: 3150.3936 - val_mae: 18.8871\n",
            "Epoch 114/200\n",
            "3240/3240 [==============================] - 44s 13ms/step - loss: 904.2983 - mae: 17.1052 - val_loss: 3164.8623 - val_mae: 19.0813\n",
            "Epoch 115/200\n",
            "3240/3240 [==============================] - 44s 13ms/step - loss: 901.1050 - mae: 17.0913 - val_loss: 3094.2788 - val_mae: 18.7755\n",
            "Epoch 116/200\n",
            "3240/3240 [==============================] - 44s 13ms/step - loss: 887.9163 - mae: 17.0323 - val_loss: 3090.7349 - val_mae: 18.8137\n",
            "Epoch 117/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 883.0366 - mae: 16.9990 - val_loss: 3124.7876 - val_mae: 18.7899\n",
            "Epoch 118/200\n",
            "3240/3240 [==============================] - 44s 13ms/step - loss: 889.1881 - mae: 17.0028 - val_loss: 3074.0356 - val_mae: 18.8084\n",
            "Epoch 119/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 880.6464 - mae: 16.9701 - val_loss: 3102.9729 - val_mae: 18.8074\n",
            "Epoch 120/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 883.7421 - mae: 16.9512 - val_loss: 3025.8254 - val_mae: 18.6680\n",
            "Epoch 121/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 879.9914 - mae: 16.9405 - val_loss: 3061.9004 - val_mae: 18.6667\n",
            "Epoch 122/200\n",
            "3240/3240 [==============================] - 44s 13ms/step - loss: 864.0809 - mae: 16.8978 - val_loss: 3153.6091 - val_mae: 19.0528\n",
            "Epoch 123/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 870.6641 - mae: 16.8627 - val_loss: 3165.6707 - val_mae: 18.7381\n",
            "Epoch 124/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 863.8823 - mae: 16.8465 - val_loss: 3133.8494 - val_mae: 18.6425\n",
            "Epoch 125/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 866.1522 - mae: 16.8291 - val_loss: 3187.8093 - val_mae: 18.8779\n",
            "Epoch 126/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 866.0833 - mae: 16.8254 - val_loss: 3177.5129 - val_mae: 18.9308\n",
            "Epoch 127/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 857.9240 - mae: 16.7775 - val_loss: 3065.0457 - val_mae: 18.6355\n",
            "Epoch 128/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 852.8625 - mae: 16.7615 - val_loss: 3080.2280 - val_mae: 18.5575\n",
            "Epoch 129/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 854.9694 - mae: 16.7465 - val_loss: 3177.8225 - val_mae: 19.0937\n",
            "Epoch 130/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 846.0917 - mae: 16.7193 - val_loss: 3141.9592 - val_mae: 18.8706\n",
            "Epoch 131/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 841.9456 - mae: 16.6670 - val_loss: 3094.8042 - val_mae: 18.5474\n",
            "Epoch 132/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 839.6158 - mae: 16.6571 - val_loss: 3086.4775 - val_mae: 18.6473\n",
            "Epoch 133/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 844.1172 - mae: 16.6568 - val_loss: 3142.2773 - val_mae: 18.6572\n",
            "Epoch 134/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 832.6180 - mae: 16.6079 - val_loss: 3068.2102 - val_mae: 18.4385\n",
            "Epoch 135/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 834.8928 - mae: 16.6229 - val_loss: 3063.7322 - val_mae: 18.4936\n",
            "Epoch 136/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 829.3365 - mae: 16.5820 - val_loss: 3103.8894 - val_mae: 18.5151\n",
            "Epoch 137/200\n",
            "3240/3240 [==============================] - 44s 13ms/step - loss: 826.9273 - mae: 16.5688 - val_loss: 3124.4202 - val_mae: 18.5940\n",
            "Epoch 138/200\n",
            "3240/3240 [==============================] - 44s 13ms/step - loss: 823.5512 - mae: 16.5464 - val_loss: 3148.0481 - val_mae: 18.5136\n",
            "Epoch 139/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 827.3326 - mae: 16.5396 - val_loss: 3132.7705 - val_mae: 18.4486\n",
            "Epoch 140/200\n",
            "3240/3240 [==============================] - 44s 13ms/step - loss: 819.8716 - mae: 16.5068 - val_loss: 3127.0510 - val_mae: 18.3783\n",
            "Epoch 141/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 819.3827 - mae: 16.4907 - val_loss: 3170.9141 - val_mae: 18.7143\n",
            "Epoch 142/200\n",
            "3240/3240 [==============================] - 44s 13ms/step - loss: 816.7603 - mae: 16.4682 - val_loss: 3079.2239 - val_mae: 18.4172\n",
            "Epoch 143/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 815.4138 - mae: 16.4438 - val_loss: 3077.7944 - val_mae: 18.3508\n",
            "Epoch 144/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 801.2681 - mae: 16.4165 - val_loss: 3088.0684 - val_mae: 18.3957\n",
            "Epoch 145/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 802.0345 - mae: 16.4052 - val_loss: 3300.2261 - val_mae: 19.2959\n",
            "Epoch 146/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 807.2759 - mae: 16.3937 - val_loss: 3050.4722 - val_mae: 18.7126\n",
            "Epoch 147/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 811.6505 - mae: 16.3872 - val_loss: 3164.4128 - val_mae: 18.9987\n",
            "Epoch 148/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 797.2198 - mae: 16.3403 - val_loss: 3013.1951 - val_mae: 18.3848\n",
            "Epoch 149/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 790.8425 - mae: 16.3148 - val_loss: 3041.0073 - val_mae: 18.2027\n",
            "Epoch 150/200\n",
            "3240/3240 [==============================] - 44s 13ms/step - loss: 797.4942 - mae: 16.3043 - val_loss: 3144.8125 - val_mae: 18.4322\n",
            "Epoch 151/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 787.1399 - mae: 16.2830 - val_loss: 3066.8125 - val_mae: 18.2026\n",
            "Epoch 152/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 797.8816 - mae: 16.2947 - val_loss: 3014.6741 - val_mae: 18.1450\n",
            "Epoch 153/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 787.1815 - mae: 16.2466 - val_loss: 3032.2588 - val_mae: 18.1889\n",
            "Epoch 154/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 789.7532 - mae: 16.2520 - val_loss: 3133.2568 - val_mae: 18.4390\n",
            "Epoch 155/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 785.7903 - mae: 16.2141 - val_loss: 3057.9912 - val_mae: 18.4518\n",
            "Epoch 156/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 785.3585 - mae: 16.2048 - val_loss: 2980.8806 - val_mae: 18.1306\n",
            "Epoch 157/200\n",
            "3240/3240 [==============================] - 44s 13ms/step - loss: 787.6406 - mae: 16.1987 - val_loss: 3069.6052 - val_mae: 18.4049\n",
            "Epoch 158/200\n",
            "3240/3240 [==============================] - 44s 13ms/step - loss: 785.0510 - mae: 16.1732 - val_loss: 3037.5854 - val_mae: 18.0507\n",
            "Epoch 159/200\n",
            "3240/3240 [==============================] - 44s 13ms/step - loss: 774.9964 - mae: 16.1603 - val_loss: 3076.1138 - val_mae: 18.2348\n",
            "Epoch 160/200\n",
            "3240/3240 [==============================] - 44s 13ms/step - loss: 768.4802 - mae: 16.1284 - val_loss: 3028.7087 - val_mae: 18.0089\n",
            "Epoch 161/200\n",
            "3240/3240 [==============================] - 44s 13ms/step - loss: 774.1163 - mae: 16.1311 - val_loss: 3049.8254 - val_mae: 18.3042\n",
            "Epoch 162/200\n",
            "3240/3240 [==============================] - 44s 13ms/step - loss: 770.5959 - mae: 16.1125 - val_loss: 3055.8977 - val_mae: 18.0829\n",
            "Epoch 163/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 773.4544 - mae: 16.0968 - val_loss: 3121.9067 - val_mae: 18.2458\n",
            "Epoch 164/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 762.1865 - mae: 16.0562 - val_loss: 3043.1624 - val_mae: 18.1102\n",
            "Epoch 165/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 758.6284 - mae: 16.0466 - val_loss: 3047.5715 - val_mae: 18.1505\n",
            "Epoch 166/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 761.7054 - mae: 16.0319 - val_loss: 3036.5198 - val_mae: 18.1636\n",
            "Epoch 167/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 763.3363 - mae: 16.0172 - val_loss: 3042.8003 - val_mae: 18.0271\n",
            "Epoch 168/200\n",
            "3240/3240 [==============================] - 44s 13ms/step - loss: 757.6718 - mae: 15.9929 - val_loss: 3016.1860 - val_mae: 18.0271\n",
            "Epoch 169/200\n",
            "3240/3240 [==============================] - 44s 13ms/step - loss: 749.2349 - mae: 15.9588 - val_loss: 3011.1047 - val_mae: 18.1073\n",
            "Epoch 170/200\n",
            "3240/3240 [==============================] - 44s 13ms/step - loss: 758.9648 - mae: 15.9950 - val_loss: 3016.6226 - val_mae: 18.1733\n",
            "Epoch 171/200\n",
            "3240/3240 [==============================] - 44s 13ms/step - loss: 759.5242 - mae: 15.9734 - val_loss: 3047.0847 - val_mae: 18.2178\n",
            "Epoch 172/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 751.5776 - mae: 15.9519 - val_loss: 2997.4607 - val_mae: 17.9619\n",
            "Epoch 173/200\n",
            "3240/3240 [==============================] - 44s 13ms/step - loss: 740.1062 - mae: 15.9118 - val_loss: 3063.3501 - val_mae: 17.9195\n",
            "Epoch 174/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 749.9685 - mae: 15.9117 - val_loss: 3021.9009 - val_mae: 17.9457\n",
            "Epoch 175/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 746.6453 - mae: 15.9166 - val_loss: 3075.6099 - val_mae: 17.9112\n",
            "Epoch 176/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 744.5826 - mae: 15.8955 - val_loss: 2977.1616 - val_mae: 17.8378\n",
            "Epoch 177/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 753.5824 - mae: 15.8933 - val_loss: 3029.3945 - val_mae: 17.9636\n",
            "Epoch 178/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 736.2715 - mae: 15.8159 - val_loss: 3013.7800 - val_mae: 17.8914\n",
            "Epoch 179/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 732.0285 - mae: 15.8203 - val_loss: 2985.5669 - val_mae: 17.8874\n",
            "Epoch 180/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 749.3940 - mae: 15.8784 - val_loss: 3072.6833 - val_mae: 18.0304\n",
            "Epoch 181/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 740.1521 - mae: 15.8268 - val_loss: 3107.3159 - val_mae: 18.6395\n",
            "Epoch 182/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 732.8655 - mae: 15.7976 - val_loss: 3008.9668 - val_mae: 17.8377\n",
            "Epoch 183/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 734.9689 - mae: 15.7766 - val_loss: 3027.2949 - val_mae: 17.8581\n",
            "Epoch 184/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 737.5137 - mae: 15.7778 - val_loss: 2998.9949 - val_mae: 17.7488\n",
            "Epoch 185/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 737.1782 - mae: 15.7524 - val_loss: 3021.9697 - val_mae: 18.0391\n",
            "Epoch 186/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 720.0798 - mae: 15.7341 - val_loss: 3001.3931 - val_mae: 17.8176\n",
            "Epoch 187/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 730.8094 - mae: 15.7414 - val_loss: 3061.0012 - val_mae: 18.3296\n",
            "Epoch 188/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 718.0207 - mae: 15.7041 - val_loss: 3053.8206 - val_mae: 17.7770\n",
            "Epoch 189/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 731.6804 - mae: 15.7362 - val_loss: 3066.9802 - val_mae: 17.8165\n",
            "Epoch 190/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 730.0743 - mae: 15.7015 - val_loss: 3052.6714 - val_mae: 18.1152\n",
            "Epoch 191/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 715.6129 - mae: 15.6654 - val_loss: 3059.6267 - val_mae: 17.8733\n",
            "Epoch 192/200\n",
            "3240/3240 [==============================] - 44s 13ms/step - loss: 722.0615 - mae: 15.6439 - val_loss: 3048.6523 - val_mae: 17.9559\n",
            "Epoch 193/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 717.0889 - mae: 15.6312 - val_loss: 3016.8467 - val_mae: 17.8128\n",
            "Epoch 194/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 713.0878 - mae: 15.6097 - val_loss: 3035.6648 - val_mae: 17.8547\n",
            "Epoch 195/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 710.1212 - mae: 15.6107 - val_loss: 3032.5835 - val_mae: 17.8594\n",
            "Epoch 196/200\n",
            "3240/3240 [==============================] - 44s 13ms/step - loss: 708.1677 - mae: 15.6003 - val_loss: 3008.3140 - val_mae: 17.6615\n",
            "Epoch 197/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 712.0612 - mae: 15.5813 - val_loss: 3048.6436 - val_mae: 17.8102\n",
            "Epoch 198/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 705.6122 - mae: 15.5632 - val_loss: 3032.8071 - val_mae: 17.6646\n",
            "Epoch 199/200\n",
            "3240/3240 [==============================] - 44s 14ms/step - loss: 702.2584 - mae: 15.5655 - val_loss: 3002.5522 - val_mae: 17.7242\n",
            "Epoch 200/200\n",
            "3240/3240 [==============================] - 44s 13ms/step - loss: 705.2064 - mae: 15.5416 - val_loss: 3091.3938 - val_mae: 18.0722\n",
            "6250/6250 [==============================] - 22s 3ms/step - loss: 3107.7268 - mae: 18.2089\n",
            "Test Loss: 3107.726806640625\n",
            "Mean Absolute Error: 18.2088565826416\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"model.h5\")\n"
      ],
      "metadata": {
        "id": "PKerQMNzOLOx",
        "executionInfo": {
          "status": "aborted",
          "timestamp": 1696713433511,
          "user_tz": -330,
          "elapsed": 1,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}